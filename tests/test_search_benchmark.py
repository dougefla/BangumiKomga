import json
import mmap
import random
import sys
import os
import time
from datetime import datetime
import unittest
from bangumi_archive.local_archive_searcher import search_all_data, _search_all_data_with_index
from bangumi_archive.archive_autoupdater import check_archive, ARCHIVE_FILES_DIR
from api.bangumi_api import BangumiApiDataSource
from config.config import BANGUMI_ACCESS_TOKEN as ACCESS_TOKEN

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° sys.pathï¼Œç¡®ä¿å¯ä»¥å¯¼å…¥æ¨¡å—
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# è¯„ä¼°é˜ˆå€¼. æš‚ç½®ä¸ºè¾ƒä½å€¼ä»¥ä¾¿é€šè¿‡æµ‹è¯•, è§‚å¯Ÿè¯„ä¼°æŠ¥å‘Š
RECALL_THRESHOLD = 0.50
TOP1_ACCURACY_THRESHOLD = 0.50

# é…ç½®
file_path = os.path.join(ARCHIVE_FILES_DIR, "subject.jsonlines")
samples_size = 100
# æ˜¯å¦è¾“å‡ºæµ‹è¯•æŠ¥å‘Šæ–‡ä»¶
is_save_report = True
show_sample_size = 5
use_token = False
if use_token:
    bgm_api = BangumiApiDataSource(ACCESS_TOKEN)
else:
    bgm_api = BangumiApiDataSource()


def sample_jsonlines(input_file, sample_size: int, output_file=None):
    if sample_size <= 0:
        raise ValueError("sample_size å¿…é¡»å¤§äº 0")
    file_size = os.path.getsize(input_file)
    if file_size == 0:
        raise ValueError("æ–‡ä»¶ä¸ºç©º")
    offsets = []
    with open(input_file, 'rb') as f:
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            pos = 0
            while pos < len(mm):
                next_pos = mm.find(b'\n', pos)
                if next_pos == -1:
                    offsets.append(pos)
                    break
                offsets.append(pos)
                pos = next_pos + 1
    total_lines = len(offsets)
    print(f"å…±æ‰¾åˆ° {total_lines} è¡Œ")
    if sample_size > total_lines:
        print(f"è­¦å‘Šï¼šè¯·æ±‚é‡‡æ · {sample_size} è¡Œï¼Œä½†æ–‡ä»¶åªæœ‰ {total_lines} è¡Œï¼Œå°†é‡‡æ ·å…¨éƒ¨è¡Œ")
        sample_size = total_lines
    sampled_indices = random.sample(range(total_lines), sample_size)
    print(f"å·²éšæœºé‡‡æ · {sample_size} è¡Œç´¢å¼•")
    samples = []
    print("æ­£åœ¨è¯»å–é‡‡æ ·è¡Œ...")
    with open(input_file, 'rb') as f:
        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
            for idx in sampled_indices:
                start = offsets[idx]
                end = offsets[idx + 1] if idx + 1 < total_lines else len(mm)
                line_bytes = mm[start:end]
                line_str = line_bytes.rstrip(b'\n\r').decode('utf-8')
                samples.append(json.loads(line_str))
    if output_file:
        with open(output_file, 'w', encoding='utf-8') as out_f:
            for item in samples:
                out_f.write(json.dumps(item, ensure_ascii=False) + '\n')
        print(f"é‡‡æ ·ç»“æœå·²å†™å…¥ {output_file}")
        return None
    else:
        return samples


def evaluate_search_function(
    data_samples,
    search_func,
    is_save_report: bool = False
):
    """
    è¯„ä¼°ä»»æ„æœç´¢å‡½æ•°çš„æ£€ç´¢æ•ˆæœ, å¹¶ç»Ÿè®¡æ£€ç´¢è€—æ—¶ã€‚
    :param data_samples: é‡‡æ ·æ•°æ®
    :param search_func: è¦æµ‹è¯•çš„æœç´¢å‡½æ•°ï¼Œå¿…é¡»æ¥å— (file_path, query) ä¸¤ä¸ªå‚æ•°
    :param is_save_report: æ˜¯å¦ä¿å­˜è¯„ä¼°ç»“æœåˆ° JSON
    """

    start_total = time.time()  # å¼€å§‹è®¡æ—¶

    # æ„å»º query-ground truth å¯¹
    query_gt_pairs = []
    for item in data_samples:
        name_cn = item.get("name_cn", "").strip()
        name = item.get("name", "").strip()
        item_id = item.get("id")
        # ç”¨ä½œå“åæ„å»ºæŸ¥è¯¢
        query = name_cn if name_cn else name
        if not query:
            continue
        query_gt_pairs.append({
            "query": query,
            "ground_truth_id": item_id,
        })
    print(f"æˆåŠŸæ„å»º {len(query_gt_pairs)} ä¸ª query-ground truth å¯¹")
    if query_gt_pairs:
        print(
            f"ç¤ºä¾‹ query: '{query_gt_pairs[0]['query']}' (ID: {query_gt_pairs[0]['ground_truth_id']})\n")

    # æ‰§è¡Œæœç´¢è¯„ä¼°
    results_per_query = []
    tp_query_count = 0  # query-level recall è®¡æ•°
    tp_total = 0        # result-level precision è®¡æ•°
    fp_total = 0
    total_queries = len(query_gt_pairs)

    # æœç´¢è€—æ—¶
    total_search_time = 0.0

    print(f"ğŸ” å¼€å§‹å¯¹æ¯ä¸ª query æ‰§è¡Œæ£€ç´¢ï¼ˆä½¿ç”¨å‡½æ•°: {search_func.__name__}ï¼‰...")
    for i, pair in enumerate(query_gt_pairs, 1):
        query = pair["query"]
        gt_id = pair["ground_truth_id"]

        # å¯¹æ¯æ¬¡æœç´¢è®¡æ—¶
        start_search = time.time()
        search_results = search_func(query)
        search_duration = time.time() - start_search
        total_search_time += search_duration

        returned_ids = [r.get("id") for r in search_results]
        found_in_results = gt_id in returned_ids
        if found_in_results:
            tp_query_count += 1
        tp_total += sum(1 for rid in returned_ids if rid == gt_id)
        fp_total += sum(1 for rid in returned_ids if rid != gt_id)

        results_per_query.append({
            "query": query,
            "gt_id": gt_id,
            "found": found_in_results,
            "search_results_count": len(returned_ids),
            "search_results_ids": returned_ids,
            "search_time": search_duration  # ä¿ç•™å•æ¬¡æœç´¢è€—æ—¶
        })

        if i % 50 == 0:
            print(
                f"  å·²å¤„ç† {i}/{total_queries}ï¼Œå·²å¬å› {tp_query_count} æ¡ï¼Œå½“å‰æœç´¢å·²è€—æ—¶: {total_search_time:.4f}s")

    # è®¡ç®—æŒ‡æ ‡
    recall = tp_query_count / total_queries if total_queries > 0 else 0.0
    precision = tp_total / \
        (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0.0
    f1 = 2 * (precision * recall) / (precision +
                                     recall) if (precision + recall) > 0 else 0.0

    # Top-1 Accuracy
    top1_correct = sum(
        1 for r in results_per_query if r["search_results_ids"] and r["search_results_ids"][0] == r["gt_id"])
    top1_accuracy = top1_correct / total_queries if total_queries > 0 else 0.0

    # è®¡æ—¶, æ€»æµç¨‹ç»“æŸ
    end_total = time.time()
    total_time = end_total - start_total

    print("\n" + "="*70)
    print("è¯„ä¼°æŠ¥å‘Š")
    print("="*70)
    print(f"æœç´¢å‡½æ•°: {search_func.__module__}.{search_func.__name__}")
    print(f"æ€»æŸ¥è¯¢æ•°: {total_queries}")
    print(f"æˆåŠŸå¬å› (TP): {tp_query_count}")
    print(f"æœªå¬å› (FN): {total_queries - tp_query_count}")
    print(
        f"å¹³å‡æ£€ç´¢ç»“æœæ•°: {sum(r['search_results_count'] for r in results_per_query) / total_queries:.2f}")
    print(f"å¬å›ç‡ (Recall): {recall:.4f} ({tp_query_count}/{total_queries})")
    print(f"ç²¾ç¡®ç‡ (Precision): {precision:.4f}")
    print(f"Top-1 å‡†ç¡®ç‡: {top1_accuracy:.4f}")
    print(f"F1-score: {f1:.4f}")
    print(f"æ€»è€—æ—¶: {total_time:.4f} ç§’")
    print(f"æœç´¢æ€»è€—æ—¶: {total_search_time:.4f} ç§’")
    print(f"å¹³å‡æ¯æ¬¡æœç´¢è€—æ—¶: {total_search_time / total_queries:.4f} ç§’")
    print("="*70)

    # é”™è¯¯æ ·ä¾‹
    failed_queries = [
        r for r in results_per_query if not r["found"]][:show_sample_size]
    print(f"\n å‰ {show_sample_size} ä¸ªæœªå¬å›çš„æŸ¥è¯¢ï¼ˆFNï¼‰:")
    for i, r in enumerate(failed_queries, 1):
        print(f"  {i}. Query: '{r['query']}' (ID: {r['gt_id']})")
        print(f"     æ£€ç´¢ç»“æœæ•°: {r['search_results_count']}")
        if r['search_results_ids']:
            ids_str = r['search_results_ids'][:3]
            suffix = "..." if len(r['search_results_ids']) > 3 else ""
            print(f"     è¿”å›çš„ ID: {ids_str}{suffix}")

    # å±•ç¤ºæœ€æ…¢çš„ 5 æ¬¡æœç´¢
    print(f"\n æœ€æ…¢çš„ {show_sample_size} æ¬¡æŸ¥è¯¢:")
    slowest_queries = sorted(
        results_per_query, key=lambda x: x["search_time"], reverse=True)[:show_sample_size]
    for i, r in enumerate(slowest_queries, 1):
        print(f"  {i}. Query: '{r['query']}' (ID: {r['gt_id']})")
        print(f"     æ£€ç´¢è€—æ—¶: {r['search_time']:.4f} ç§’")
        print(f"     æ£€ç´¢ç»“æœæ•°: {r['search_results_count']}")
        if r['search_results_ids']:
            ids_str = r['search_results_ids'][:3]
            suffix = "..." if len(r['search_results_ids']) > 3 else ""
            print(f"     è¿”å›çš„ ID: {ids_str}{suffix}")

    # ä¿å­˜æŠ¥å‘Š
    if is_save_report:
        output_eval = {
            "total_queries": total_queries,
            "tp_count": tp_query_count,
            "recall": recall,
            "precision": precision,
            "f1": f1,
            "top1_accuracy": top1_accuracy,
            "search_function": f"{search_func.__module__}.{search_func.__name__}",
            "total_time_seconds": total_time,
            "search_total_time_seconds": total_search_time,
            "avg_search_time_seconds": total_search_time / total_queries,
            "failed_queries": [
                {
                    "query": r["query"],
                    "gt_id": r["gt_id"],
                    "search_results_count": r["search_results_count"],
                    "search_results_ids": r["search_results_ids"],
                    "search_time": r.get("search_time", 0.0)
                }
                for r in failed_queries
            ],
            "slowest_queries": [
                {
                    "query": r["query"],
                    "gt_id": r["gt_id"],
                    "search_results_count": r["search_results_count"],
                    "search_results_ids": r["search_results_ids"],
                    "search_time": r["search_time"]
                }
                for r in slowest_queries
            ]
        }
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs("test_results", exist_ok=True)
        eval_file = f"test_results/search_func_eval_results_{timestamp}.json"
        with open(eval_file, 'w', encoding='utf-8') as f:
            json.dump(output_eval, f, ensure_ascii=False, indent=2)
        print(f"\n æ£€ç´¢å‡½æ•°è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³: {eval_file}")

    return {
        "recall": recall,
        "precision": precision,
        "f1": f1,
        "top1_accuracy": top1_accuracy,
        "search_function": f"{search_func.__module__}.{search_func.__name__}",
        "total_queries": total_queries,
        "tp_count": tp_query_count,
        "total_time_seconds": total_time,
        "search_total_time_seconds": total_search_time,
        "avg_search_time_seconds": total_search_time / total_queries
    }


class TestSearchFunctionEvaluation(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        print("\n æ­£åœ¨å‡†å¤‡ Bangumi Archive æ•°æ®æ–‡ä»¶...")
        # ä½¿é‡‡æ ·ç»“æœå¯å¤ç°
        # random.seed(42)
        try:
            check_archive()
            # éªŒè¯æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”éç©º
            if not os.path.exists(file_path):
                raise FileNotFoundError(f"Archive æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            if os.path.getsize(file_path) == 0:
                raise ValueError(f"Archive æ–‡ä»¶ä¸ºç©º: {file_path}")
            print(f" Archive æ–‡ä»¶å‡†å¤‡å®Œæˆ: {file_path}")

            # é‡‡æ ·æ”¾åˆ°setUpClassä»¥ä¾¿æµ‹è¯•å…±äº«åŒä¸€ä»½é‡‡æ ·æ•°æ®
            cls.sampled_data = sample_jsonlines(file_path, samples_size)
            if not cls.sampled_data:
                raise ValueError("é‡‡æ ·ç»“æœä¸ºç©º")
            print(f"é‡‡æ ·å®Œæˆï¼Œå…± {len(cls.sampled_data)} ä¸ªæ ·æœ¬")
        except Exception as e:
            raise unittest.SkipTest(f" Archive å‡†å¤‡å¤±è´¥ï¼Œè·³è¿‡æµ‹è¯•: {str(e)}")

    def test_offline_search_function(self):
        """æµ‹è¯•æ£€ç´¢å‡½æ•°çš„å¬å›ç‡å’ŒTop-1å‡†ç¡®ç‡æ˜¯å¦è¾¾æ ‡"""

        def search_func_offline(
            query): return _search_all_data_with_index(file_path, query)
        try:
            metrics = evaluate_search_function(
                data_samples=self.__class__.sampled_data,
                search_func=search_func_offline,
                is_save_report=is_save_report
            )
        except Exception as e:
            self.fail(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")

        # è¾“å‡ºæŒ‡æ ‡åˆ° stdoutï¼Œä¾› CI æ•è·
        print(json.dumps(metrics, ensure_ascii=False, indent=None))

        # æ–­è¨€é˜ˆå€¼
        self.assertGreaterEqual(
            metrics["recall"],
            RECALL_THRESHOLD,
            f"å¬å›ç‡ {metrics['recall']:.4f} ä½äºé˜ˆå€¼ {RECALL_THRESHOLD}"
        )
        self.assertGreaterEqual(
            metrics["top1_accuracy"],
            TOP1_ACCURACY_THRESHOLD,
            f"Top-1 å‡†ç¡®ç‡ {metrics['top1_accuracy']:.4f} ä½äºé˜ˆå€¼ {TOP1_ACCURACY_THRESHOLD}"
        )

    def test_online_search_function(self):
        """æµ‹è¯•æ£€ç´¢å‡½æ•°çš„å¬å›ç‡å’ŒTop-1å‡†ç¡®ç‡æ˜¯å¦è¾¾æ ‡"""
        def search_func_online(query):
            # 1 RPS,ä½¿æµ‹è¯•çš„è¯·æ±‚é€Ÿç‡ä½äºé™æµå™¨è¦æ±‚
            time.sleep(1)
            return bgm_api.search_subjects(query)
        try:
            metrics = evaluate_search_function(
                self.__class__.sampled_data,
                search_func=search_func_online,
                is_save_report=is_save_report
            )
        except Exception as e:
            self.fail(f"è¯„ä¼°è¿‡ç¨‹å‡ºé”™: {str(e)}")

        # è¾“å‡ºæŒ‡æ ‡åˆ° stdoutï¼Œä¾› CI æ•è·
        print(json.dumps(metrics, ensure_ascii=False, indent=None))

        # æ–­è¨€é˜ˆå€¼
        self.assertGreaterEqual(
            metrics["recall"],
            RECALL_THRESHOLD,
            f"å¬å›ç‡ {metrics['recall']:.4f} ä½äºé˜ˆå€¼ {RECALL_THRESHOLD}"
        )
        self.assertGreaterEqual(
            metrics["top1_accuracy"],
            TOP1_ACCURACY_THRESHOLD,
            f"Top-1 å‡†ç¡®ç‡ {metrics['top1_accuracy']:.4f} ä½äºé˜ˆå€¼ {TOP1_ACCURACY_THRESHOLD}"
        )
